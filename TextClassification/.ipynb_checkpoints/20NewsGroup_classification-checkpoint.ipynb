{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "emerging-fusion",
   "metadata": {},
   "source": [
    "# Importing required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "remarkable-nirvana",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-bennett",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designing-springfield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data target_name:\n",
      " ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Data:  4162\n"
     ]
    }
   ],
   "source": [
    "# list of categories for training model\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'talk.politics.misc', 'comp.graphics', 'sci.space']\n",
    "news_group_data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, remove=['headers', 'footers', 'quotes'], random_state=42)\n",
    "# display\n",
    "print(\"Data target_name:\\n\", news_group_data.target_names)\n",
    "print(\"Data: \", len(news_group_data['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worth-trouble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('alt.atheism', 799), ('comp.graphics', 973), ('sci.space', 987), ('talk.politics.misc', 775), ('talk.religion.misc', 628)]\n"
     ]
    }
   ],
   "source": [
    "# calculate frequency\n",
    "target, count_target = np.unique(news_group_data.target, return_counts=True)\n",
    "target_names = np.array(news_group_data.target_names)\n",
    "print(list(zip(target_names, count_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-patrol",
   "metadata": {},
   "source": [
    "# Converting into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "raising-stationery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nPeter Nelson posted a very eloquent response...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nNo.\\n\\n\\nNo. The library at Alexandria was p...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forwarded from:\\nPUBLIC INFORMATION OFFICE\\nJE...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can someone tell me where i could find ansi or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  target\n",
       "0  \\nPeter Nelson posted a very eloquent response...       3\n",
       "1  \\nNo.\\n\\n\\nNo. The library at Alexandria was p...       4\n",
       "2  \\nWouldn't this require a hyper-sphere.  In 3-...       1\n",
       "3  Forwarded from:\\nPUBLIC INFORMATION OFFICE\\nJE...       2\n",
       "4  can someone tell me where i could find ansi or...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df = pd.DataFrame({'data':news_group_data.data, 'target':news_group_data.target})\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reserved-floating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4162"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of data\n",
    "len(complete_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-management",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "    - WordNetLemmatizer() - removing all the special characters, spaces, character from start, and also converting into lower case\n",
    "    - removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foster-eligibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['peter nelson posted eloquent response point talk politics misc need consume bandwidth meant clear intersection set liberal libertarian philosophy natural right government constitutional interpretation particular fit philosophy philosophy engages serious practical error endowing nine lawyer supreme court almost totalitarian authority completely outside consent consensus people supreme court nomination amazing political fist fight day control court rule country people court may well trying best job best benevolent oligarch even approve every supreme court decision ever eventually oligarch arise decimate hold dear try supreme court case jury problem would mitigated great deal would create broad non enumerated government power level european parlamentary democracy current de facto standard essentially engaged fundamental mistake except different body totalitarian virtually unchecked except plurality election death retirement government power like asking wind blow unless prove fact engaging certain activity absolutely effect whatsoever human consented engagement activity human activity indeed fit category even would empower make judgement fully autonomous activity defines picking pocket defrauding economic asset person asset peace mind stability confidence child emotional environment security many thing also part person asset give right create moral environment parent strongly object give right create environment social unrest instability say effect authority say empowered make value judgement federal constitution explicitly prohibited federal government would prevent could muster enough local support pas amendment local constitution say 3 4 majority empowering local government would vote foot move neighborhood friendly system value ideal situation far better mess mired right define right broadly practical choice whether people infringe upon right since right overlap even right defined narrowly government empowered prevent others infringing right fundamental question whose authority power created support current situation natural right supreme court rather original understanding supreme court even better supreme court jury consenting nine lawyer washington c create power air fractal federalism scenario broad consensus people e amendment process creates power mean federal government come trucking gun tell local run neighborhood waco tx nice example create community public masturbation permitted cause personal autonomy done anything different precisely autonomous activity referring list perhaps get enough people agree truly autonomous pas constitutional amendment protecting koresh incident appears horrendous abuse government power power possibly illegitimately obtained mean would abolish constitutionality knock warrant seems dubious mention mere existence batf government propensity ignore word infringe second amendment power upheld body incredible concentration power hand nine people upheld much called civil right read affirmative action legislation despite fourteenth amendment money certainly asset world would bleak existence indeed matt freivald liborgalism thinking irrelevant integrity irrelevant free speech irrelevant private property irrelevant personal responsibility irrelevant conservativism futile assimilated opinion employer', 'library alexandria perhaps greatest library ever built world greek love wisdom philo sophos great love reflected alexandrian library christian got hold began modifying purging text moslem invaded either christian burned library keep falling moslem hand far likely since book burner moslem burned sack city moslem burned either way tremendous amount information lost destruction library alexandria probably one greatest crime man man actually hebrew almah young woman translated hellenistic greek parthenos may may correctly translated modern technical english term virgin jew type virginity cult greco roman artemis diana standard text used christian jew masoretic text jew course use text original hebrew without translation propaganda']\n"
     ]
    }
   ],
   "source": [
    "# creating corpus list for iterating over text feature from data\n",
    "corpus = list() \n",
    "# iterating over text data\n",
    "for texts in range(0, len(complete_df)):\n",
    "    # remove special character\n",
    "    input_text = re.sub(r'\\W', ' ', complete_df['data'][texts])\n",
    "    # remove single character\n",
    "    input_text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', input_text)\n",
    "    # removing single character from start\n",
    "    input_text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', input_text)\n",
    "    # multiple spaces into single spaces\n",
    "    input_text = re.sub(r'\\s+', ' ', input_text, flags=re.I)\n",
    "    # removing prefixes\n",
    "    input_text = re.sub(r'^b\\s+', '', input_text)\n",
    "    # into lower case\n",
    "    input_text = input_text.lower()\n",
    "    input_text = input_text.split()\n",
    "    # initialise the WordNetLemmatizer()\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    input_text = [stemmer.lemmatize(word) for word in input_text if not word in set(stopwords.words('english'))]\n",
    "    input_text = ' '.join(input_text)\n",
    "    # append list\n",
    "    corpus.append(input_text)\n",
    "\n",
    "# sanity check\n",
    "print(corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-impression",
   "metadata": {},
   "source": [
    "# Bag of Words Model\n",
    "    - TF-IDF (Term Frequency - Inverse Document frequency) - Convert a collection of raw documents to a matrix of TF-IDF features \n",
    "    - CountVectoirzer - Convert a Collection of text documents to a matrix of token counts\n",
    "    \n",
    "Note: Chossing CountVectorizer() for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wound-defense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', ',', 'how', 'have', 'you', 'been', '!', '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define tokenizer function\n",
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    Function to split the sentence into individual words.\n",
    "    Args:\n",
    "    Sentence: document containing Sentence (str)\n",
    "    Returns:\n",
    "    List of split individual words (Token)\n",
    "    \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# sanity check\n",
    "sentence = \"Hi there, how have you been!?\"\n",
    "tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "therapeutic-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating TF-IDF\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), tokenizer=tokenizer, max_features=1000000, use_idf=True)\n",
    "# X = tfidf_vectorizer.fit_transform(corpus).toarray() # corpus is complete_df['data']\n",
    "# y = complete_df.iloc[:, -1].values # labbels as target feature\n",
    "# print(len(X[0])) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "utility-burden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "Input feature's Shape:  (4162, 100000)\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model\n",
    "count_vetorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2), tokenizer=tokenizer, max_features=100000) # 10000\n",
    "X = count_vetorizer.fit_transform(corpus).toarray() # corpus is complete_df['data']\n",
    "y = complete_df.iloc[:, -1].values # labbels as target feature\n",
    "print(len(X[0]))\n",
    "print(\"Input feature's Shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-disco",
   "metadata": {},
   "source": [
    "# Splitting the dataset into the Training and Test set\n",
    "    - 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "determined-characterization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (3329, 100000)\n",
      "x_test:  (833, 100000)\n",
      "y_train:  (3329,)\n",
      "y_test:  (833,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "print(\"x_train: \", x_train.shape)\n",
    "print(\"x_test: \", x_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-integral",
   "metadata": {},
   "source": [
    "# Define model\n",
    "    - Naive Bayes - MultinomialNB()\n",
    "    - SVM - Linear SVC\n",
    "    - Neural Network - MLPClassifier()\n",
    "    - comparing with Ensemble - Votting Classifier (hard votting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lucky-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "# define function - model_ensembler()\n",
    "def model_ensembler():\n",
    "    # model list\n",
    "    models = list()\n",
    "    # define models - SVM, Naive Bayes, NN\n",
    "    models.append(('SVM', SVC(gamma='auto', kernel='linear')))\n",
    "    models.append(('Naive Bayes', MultinomialNB()))\n",
    "    models.append(('NN', MLPClassifier(alpha=1, max_iter=1000, hidden_layer_sizes=(2, 4, 6), solver='lbfgs', random_state=32)))\n",
    "    # define votting classifier\n",
    "    hard_voting_ensemble = VotingClassifier(estimators=models, voting='hard') # with largest sum of votes from models\n",
    "    hard_voting_ensemble = hard_voting_ensemble.fit(x_train, y_train)\n",
    "    \n",
    "    return hard_voting_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-forwarding",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-baptist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models():\n",
    "    # empty dict\n",
    "    models = dict()\n",
    "    # define model for evaluation\n",
    "    models['SVM'] = SVC(gamma='auto', kernel='linear')\n",
    "    models['Naive Bayes'] = MultinomialNB()\n",
    "    models['NN'] = MLPClassifier(alpha=1, max_iter=1000, hidden_layer_sizes=(2, 4, 6), solver='lbfgs', random_state=32) \n",
    "    models['Hard Voting Ensembler'] = model_ensembler()\n",
    "    return models\n",
    "\n",
    "# evaluation using Cross-validation - RepeatedStratifiedKFold()\n",
    "def cross_validation_evaluation(model, x_train, y_train):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, random_state=1, n_repeats=3)\n",
    "    scores=cross_val_score(model, x_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy', error_score='raise')\n",
    "    return scores\n",
    "\n",
    "# get the models evaluation\n",
    "models = evaluate_models()\n",
    "# empty lists\n",
    "results, model_names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = cross_validation_evaluation(model, x_train, y_train)\n",
    "    results.append(scores)\n",
    "    model_names.append(name)\n",
    "    print('>>>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-interface",
   "metadata": {},
   "source": [
    "# Model comparison\n",
    "    - Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(14, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.boxplot(results, labels=model_names)\n",
    "plt.title(\"Classification model's performance comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-imperial",
   "metadata": {},
   "source": [
    "# Choosing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-gamma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-landing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-boost",
   "metadata": {},
   "source": [
    "# Saving & loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# saving model\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "loading_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-indian",
   "metadata": {},
   "source": [
    "# Evaluation metrics\n",
    "    - Confusion metrix\n",
    "    - Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-effects",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-today",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "finnish-sharing",
   "metadata": {},
   "source": [
    "# Recocomender \n",
    " - Future work (Recommender based on texts)\n",
    " - link: https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
